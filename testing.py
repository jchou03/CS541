def question_3_1(file_path: str) -> str:
    return "hello world"

def question_3_2(text: str, n: int):
    return "hello world"

def question_3_3(ngram_counts: dict,  vocab_size: int) -> dict:
    return {}

def question_3_4(text: str, pc: float, vocab: list, seed: int) -> str:
    return "hello world"

def question3_5(bigram_counts: dict, vocab: list):
    return None

def question3_6(vocab_size: int, corruption_prob: float):
    return None

def question3_7(text: str, vocab: list) -> list:
    return []

def question3_8(indices: list, vocab: list) -> str:
    return "hello world"

def question3_9(indices: list, vocab_size: int):
    return None

def question3_10(original_text: str,bigram_counts:dict, vocab: list, pc: float):
    return None

def question3_11(original_text: str, recovered_text: str) -> float:
    return 0

def question_4_2(transform, batch_size:int, shuffle:bool, drop_last:bool):
    return None

def question_5_2_train_one_epoch(model, trainloader, device, optimizer, criterion, batch_size, flatten):
    return None

def question_5_3_compute_accuracy(logits, labels, batch_size: int):
    return 0

def question_5_4_evaluate(model, testloader, criterion, batch_size, device, flatten: bool):
    return None

def question_5_5_train_model(model, device, num_epochs, batch_size, trainloader, testloader, flatten: bool = False):
    return None

class MyMLP:
    def __init__(self):
        print("hello")

class MyCNN:
    def __init__(self):
        print("hello")