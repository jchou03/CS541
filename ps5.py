# -*- coding: utf-8 -*-
"""ps5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kgl5Q4yoCGKDRaWlJV2dLLDoz9OxlmXo

# CS541: Applied Machine Learning, Fall 2024, Problem Set 5

### _Note: Do not delete or add blocks, do not change the function names. If you do this the autograder won't run properly and you might get a 0. Avoid using global variables inside your function. Failing to define your variables correctly could result in a score of 0._


Problem set 5 is due in Gradescope on **Nov 19 at 11:59pm**.
All the questions are in this jupyter notebook file. There are four questions in this assignment, each of which could have multiple parts and consists of a mix of coding and short answer questions. This assignment is worth a total of **100 points** (**82.5 pts** coding, and **17.5 pts** short answer).  There is a bonus question at the end which is worth an extra 10 pts but your maximum final score will be capped at 100 if you scored beyond 100. Note that each individual pset contributes the same amount to the final grade regardless of the number of points it is worth.

After completing these questions you will need to covert this notebook into a .py file named **ps5.py** and a pdf file named **ps5.pdf** in order to submit it (details below).

**Submission instructions:** please upload your completed solution files to Gradescope by the due date. **Make sure you have run all code cells and rendered all markdown/Latex without any errors.**

There will be two separate submission links for the assignment:
1. Submit **ps5.py** to `PS5-Code`
2. Submit a single `.pdf` report that contains your work for all written questions to `PS5`. You can type your responses in LaTeX, or any other word processing software.  You can also hand write them on a tablet, or scan in hand-written answers. If you hand-write, please make sure they are neat and legible. If you are scanning, make sure that the scans are legible. Lastly, convert your work into a `PDF`. You can use Jupyter Notebook to convert the formats:
  + Convert to PDF file: Go to File->Download as->PDF
  + Convert py file: Go to File->Download as->py\
You can take a look at an example [here](https://raw.githubusercontent.com/chaudatascience/cs599_fall2022/master/ps1/convert_py.gif)

  Your written responses in the PDF report should be self-contained. It should include all the output you want us to look at. You will not receive credit for any results you have obtained, but failed to include directly in the PDF report file.  Please tag the reponses in your PDF with the Gradescope questions outline  as described in [Submitting an Assignment](https://youtu.be/u-pK4GzpId0).

**Assignment Setup**

You are strongly encouraged to use [Google Colab](https://colab.research.google.com/) for this assignment.

If you would prefer to setup your code locally on your own machine, you will need [Jupyter Notebook](https://jupyter.org/install#jupyter-notebook) or [JupyterLab](https://jupyter.org/install#jupyterlab) installation. One way to set it up is to install “Anaconda” distribution, which has Python (you should install python version >= 3.9 as this notebook is tested with python 3.9), several libraries including the Jupyter Notebook that we will use in class. It is available for Windows, Linux, and Mac OS X [here](https://docs.conda.io/en/latest/miniconda.html).

If you are not familiar with Jupyter Notebook, you can follow [this blog](https://realpython.com/jupyter-notebook-introduction/) for an introduction.  After developing your code using Jupyter, you are encouraged to test it on Google Colab to ensure it works in both settings.


You cannot use packages other than the ones already imported in this assignment.

**Jupyter Tip 1**: To run a cell, press `Shift+Enter` or click on "play" button above. To edit any code or text cell [double] click on its content. To change cell type, choose "Markdown" or "Code" in the drop-down menu above.

**Jupyter Tip 2**: Use shortcut "Shift + Tab" to show the documentation of a function in Jupyter Notebook/ Jupterlab. Press Shift then double Tab (i.e., press Tab twice) to show the full documentation.\
For example, type `sum(` then Shift + Tab to show the documentation for the function, as shown in this the picture below.
"""

## import some libraries
import numpy as np
from typing import Tuple, List, Dict
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple
from IPython.display import display
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset

"""# **Question 1.** Autoencoder (*45 total points*)

In this question, you will build and train an autoencoder model using the MNIST dataset. The MNIST dataset is a collection grayscale images of handwritten digits ranging from 0 to 9 commonly used for image classification tasks.

An autoencoder is a neural network that learns to compress the input data into a lower-dimensional representation (encoding) and then reconstruct it back to its original form (decoding). It consists of the following two major components:

**Encoder**: compresses the input images into lower-dimensional representation.

**Decoder**: takes the encoded representation and reconstructs an output as close as possible to the original input image.

## **1.1 Code:** Data Transformation *(2.5 pts)*

For the following sections, we will work on [MNIST](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) dataset.



First, let's download and preprocess the dataset
"""

def question_1_1() -> transforms.Compose:
    """
    Create and return a transformation pipeline for image preprocessing.
    """
    # Write your code in this block -----------------------------------------------------------

    ## Step 1: Use transforms.Compose
    # 1. resize images to 28x28 pixels.
    # 2. convert the image to a PyTorch tensor.
    transform = transforms.Compose([
        transforms.Resize((28, 28)),
        transforms.ToTensor()
    ])

    ## Return the transformation pipeline
    return transform
    # End of your code -----------------------------------------------------------

"""We will now create the dataloaders for training and testing."""

# Load the MNIST dataset for training and testing using batch size of 16
transform = question_1_1()
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)

"""## **1.2 Code:** Constructing Autoencoder *(5 pts)*"""

import torch.nn as nn
from torch import Tensor

class Autoencoder(nn.Module):
    def __init__(self, encoding_dim: int):

        super().__init__()

        # Write your code in this block -----------------------------------------------------------

        # Initialize self.encoder and self.decoder based on the given output
        # (note the dimension for in_features and out_features)

        # Construct encoder using following steps:
        # 1. Apply a linear layer with 784 input features and 128 output features.
        # 2. Add a ReLU activation function.
        # 3. Apply a second linear layer that reduces the representation from 128 features to encoding_dim
        self.encoder = nn.Sequential(
            nn.Linear(784, 128),
            nn.ReLU(),
            nn.Linear(128, encoding_dim)
        )

        # Construct decoder using following steps:
        # 1. Apply a linear layer with encoding_dim features and 128 output features.
        # 2. Add a ReLU activation function.
        # 3. Apply a second linear layer from 128 features to 784 features
        # 4. Apply a sigmoid layer.
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 784),
            nn.Sigmoid(),
        )

    def forward(self, x: Tensor) -> Tensor:
        """
        Forward pass for the autoencoder.
        x: Input tensor of shape (batch_size, 1, 28, 28).
        Returns the reconstructed tensor of the same shape (batch_size, 1, 28, 28)..
        """
        # Flatten the input image to shape (batch_size,28*28)
        x = self.encoder(x.view(x.size(0), -1)) # reshape input image into flattened shape
        x = self.decoder(x)
        return x.view(x.size(0), 1, 28, 28) # reshape linear output into image shape (batch_size, 1, 28, 28)
        # End of your code ------------------------------------------------------------------------

# Example of instantiating the model
encoding_dim = 64
model = Autoencoder(encoding_dim)
print(model)
# Autoencoder(
#   (encoder): Sequential(
#     (0): Linear(in_features=784, out_features=128, bias=True)
#     (1): ReLU()
#     (2): Linear(in_features=128, out_features=64, bias=True)
#   )
#   (decoder): Sequential(
#     (0): Linear(in_features=64, out_features=128, bias=True)
#     (1): ReLU()
#     (2): Linear(in_features=128, out_features=784, bias=True)
#     (3): Sigmoid()
#   )
# )

"""For training we need to first create `model` and set its device. We will use GPU if it's available, otherwise CPU.  For `optimizer` we will use Adam and set the initial learning rate as 0.001."""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
encoding_dim = 64
model = Autoencoder(encoding_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
print(device)

"""## **1.3 Code:** Training Autoencoder *(5 pts)*

Now, let us write the function to train an autoencoder model.
"""

def question_1_3(model: nn.Module, dataloader: DataLoader, criterion, optimizer, epochs) -> None:
    """
    Train an autoencoder model on a given dataset.

    Args:
    - model (nn.Module): The autoencoder model to be trained
    - dataloader (DataLoader): Dataloader for trianing
    - criterion: Loss function
    - optimizer: Optimizer
    - epochs: NUmber of trianing epochs

    """


    # Write your code in this block -----------------------------------------------------------
    ## Set training mode
    model.train()

    ## Loop through each epoch
    for epoch in range(epochs):
        total_loss = 0.0
        for images, _ in dataloader:
            images = images.to(device)

            # Forward pass

            # Zero the gradients, backpropagate, and update the weights
            model.zero_grad() # zero gradients for the next pass
            outputs = model(images)
            loss = criterion(outputs, images)
            loss.backward()
            optimizer.step()

            # Update the loss
            total_loss += loss.item()

        # Compute average loss over the epoch  and print out the loss
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")
    # End of your code ------------------------------------------------------------------------

"""## **1.4 Short answer:**  Deciding Loss Function *(2.5 pts)*

When training the model, experiment with different loss functions as the criterion. Choose the most suitable loss function for the task and explain why it is the best choice.
"""

# Write your code in this block -----------------------------------------------------------
# invoke question_1_3 here, try out at least 2 loss functions, and leave one for the test set below



criterion = nn.BCELoss()
print(criterion)
train_model = question_1_3(model, train_loader, criterion, optimizer, epochs=5)

"""Write your answer in this block

**Answer:**

Now, let us evaluate the performance of the autoencoder using the test set.
"""

def evaluate_model(model: nn.Module, dataloader: DataLoader) -> float:
    """
    Evaluate an autoencoder model on a given dataset.

    Args:
    - model (nn.Module): The autoencoder model to be evaluated
    - dataloader (DataLoader): Dataloader for evaluation

    Returns:
    - float: Average loss over the evaluation dataset
    """

    ## Set the model to evaluation mode
    model.eval()
    total_loss = 0.0

    ## Disable gradient calculation
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            outputs = model(images)
            loss = criterion(outputs, images)

    # Calculate total_loss
            total_loss += loss.item()

    # Calculate average loss over the entire evaluation dataset and print out the loss
    avg_loss = total_loss / len(dataloader)
    print(f"Test Loss: {avg_loss:.4f}")



    return avg_loss

question_1_3(model, train_loader, criterion, optimizer, epochs=5)
evaluate_model(model, test_loader)
# Epoch [1/5], Loss: 0.0206
# Epoch [2/5], Loss: 0.0086
# Epoch [3/5], Loss: 0.0066
# Epoch [4/5], Loss: 0.0055
# Epoch [5/5], Loss: 0.0049
# Test Loss: 0.0046
# 0.00462578858807683

"""To visualize the performance of the current autoencoder run the following block:"""

import matplotlib.pyplot as plt
dataiter = iter(test_loader)
images, labels = next(dataiter)

images_flatten = images.view(images.size(0), -1)
model = model.to("cpu") # added line to convert model to cpu
output = model(images_flatten)
images = images.numpy()


output = output.view(16, 1, 28, 28)
output = output.detach().numpy()

fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25, 4))

row_titles = ["Actual", "Predicted"]
for images_set, row, title in zip([images, output], axes, row_titles):
    for img, ax in zip(images_set, row):
        ax.imshow(np.squeeze(img), cmap='gray')
        ax.set_title(title, fontsize=18)  # Set "Actual" or "Predicted" as title for each subfigure
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

plt.tight_layout()
plt.show()

"""## **1.5 Code:** Adding Layers to Autoencoder *(10 pts)*
Try **three** different ways of adding a layer to the encoder and a layer to the decoder and retrain it. Do they get better? Discuss what you learn from the results.
"""

## -- ! code required --your solution(s)
class Autoencoder_new(nn.Module):
    def __init__(self, encoding_dim):
        super(Autoencoder_new, self).__init__()

        # Try different ways of adding layers to your encoder and decoder
        self.encoder = nn.Sequential(
        # Write your code in this block -----
          nn.Linear(784, 512),
          nn.ReLU(),
          nn.Linear(512, 128),
          nn.ReLU(),
          nn.Linear(128, encoding_dim),
        # End of your code --------
        )
        self.decoder = nn.Sequential(
        # Write your code in this block -----
          nn.Linear(encoding_dim, 128),
          nn.ReLU(),
          nn.Linear(128, 512),
          nn.ReLU(),
          nn.Linear(512, 784),
          nn.Sigmoid(),
        # End of your code --------
        )

    def forward(self, x):
        # Write your code in this block -----
        x = self.encoder(x.view(x.size(0), -1))
        x = self.decoder(x)
        decoded = x.view(x.size(0), 1, 28, 28)
        # End of your code --------
        return decoded


encoding_dim = 64
model_new = Autoencoder_new(encoding_dim).to(device)
# model_new = Autoencoder_new(encoding_dim) original
print(model_new)

criterion = nn.BCELoss()
optimizer = optim.Adam(model_new.parameters(), lr=0.001)


question_1_3(model_new, train_loader, criterion, optimizer, epochs=10)
# train_model(model_new, train_loader, criterion, optimizer, epochs=5)
evaluate_model(model_new, test_loader)

dataiter = iter(test_loader)
images, labels = next(dataiter)

images_flatten = images.view(images.size(0), -1)
model_new = model_new.to("cpu") # added line to convert model to cpu
output = model_new(images_flatten)
images = images.numpy()


output = output.view(16, 1, 28, 28)
output = output.detach().numpy()

fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25, 4))

row_titles = ["Actual", "Predicted"]
for images_set, row, title in zip([images, output], axes, row_titles):
    for img, ax in zip(images_set, row):
        ax.imshow(np.squeeze(img), cmap='gray')
        ax.set_title(title, fontsize=18)  # Set "Actual" or "Predicted" as title for each subfigure
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

plt.tight_layout()
plt.show()

"""Write your answer in this block

**Answer:**

## **1.6 Code:** Decoding Training Data *(10 pts)*

Determine the mean and covariance of the samples from our training data. Now
draw 10 random samples from a normal distribution with that mean and
covariance, and feed these samples into the decoder. Do the results look
like images?
"""

## -- ! code required --your solution(s)

# Step 1: Collect samples from training data
transform = question_1_1()
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(dataset=train_dataset,shuffle=True, batch_size=16)

# this stores everything in memory causing memory overload issues
# samples = torch.tensor([])

# for images, _ in train_loader:
#   flattened_images = images.view(images.size(0), -1)
#   encoded_images = model.encoder(flattened_images)
#   # append to list of samples
#   samples = torch.cat((samples, encoded_images), 0)

# print(samples.shape)

# Step 2: Calculate mean and covariance of the input samples
# need to calculate mean and cov of encoded input samples

means = None
cov = None
num_samples = 0

for images, _ in train_loader:
  # run encoder on images
  images = images.view(images.size(0), -1)
  batch_size = images.size(0)

  encoded_images = model.encoder(images)
  # print(encoded_images.shape)

  # calculate mean and covariance of encoded images
  batch_means = torch.mean(encoded_images, dim=0)
  batch_cov = torch.cov(encoded_images.t())

  if means == None:
    means = batch_means
    cov = batch_cov
  else:
    means += (batch_means - means) * (batch_size / (num_samples + batch_size))
    cov = ((cov * num_samples) + (batch_cov * batch_size)) / (num_samples + batch_size)

  num_samples += images.size(0)

print(f'means: {means.shape}')
print(means)
print(f'cov: {cov.shape}')
print(cov)

# # Step 3: Sample from the normal distribution with calculated mean and covariance
num_samples = 10
stds = torch.sqrt(torch.diag(cov))
samples = []

for i in range(num_samples):
  sample = torch.normal(mean=means, std=stds)
  samples.append(sample)

# conver to tensor
samples = torch.stack(samples, dim=0)
print(samples.shape)

# Step 4: Decode samples and visualize
decoded_samples = model.decoder(samples).view(num_samples, 1, 28, 28)

# Plot the generated images
fig, axes = plt.subplots(2, 5, figsize=(10, 5))

axes = axes.flatten()

for i in range(num_samples):
  axes[i].imshow(np.squeeze(decoded_samples[i].detach().numpy()), cmap='gray')
  axes[i].set_title(f"Sample {i+1}", fontsize=18)
  axes[i].axis('off')

plt.tight_layout()
plt.show()



"""Write your answer in this block

**Answer:**

## **1.7 Code:** Decoding using Mixture of Normals *(10 pts)*

We will now model the images using a mixture of normals. For each digit class, determine the mean and covariance based on the training data.
Your mixture distribution that samples evenly from the ten class distributions. Now draw 10 random samples from this mixture distribution,
and feed these samples into the decoder. Do the results look like images?
"""

## -- ! code required --your solution(s)

# Step 1: Calculate mean and covariance for each class
transform = question_1_1()
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(dataset=train_dataset,shuffle=True, batch_size=1)

# use dict to store the means and covariance for each class
means = {}
covs = {}
num_samples = 0.0

# first calculate means
for image, label in train_loader:
  # run encoder on images
  # print(image.shape)
  image = image.view(image.size(0), -1).to(device)
  # this should be 1 in this sample
  batch_size = image.size(0)
  # print(label)
  l = label.item()
  # print(image.shape)
  encoded_image = model.encoder(image)
  # print(encoded_image.shape)

  if l not in means:
    # if no mean/cov has been calculated yet, then
    means[l] = encoded_image[0]
    covs[l] = 0.0
  else:
    # since processing one input at a time, no need to calculate batch mean
    batch_mean = encoded_image[0]
    diff = encoded_image - means[l]

    batch_cov = torch.mm(diff.t(), diff)

    # printing outputs to check that it is working
    # print(f'batch cov: {batch_cov}')
    # print(f'batch cov shape: {batch_cov.shape}')
    means[l] += (batch_mean - means[l]) * (batch_size / (num_samples + batch_size))
    covs[l] = ((covs[l] * num_samples) + (batch_cov * batch_size)) / (num_samples + batch_size)
  num_samples += batch_size

# Step 2: Calculate the mean and covariance for each class

print(means.keys())
print(covs.keys())

print(means)
print(covs)

# Step 3: Create the mixture model and draw the samples
num_samples = 10
samples = []

for i in range(num_samples):
  sample = np.random.multivariate_normal(means[i].cpu().detach().numpy(), covs[i].cpu().detach().numpy())
  samples.append(torch.tensor(sample, dtype=torch.float32))

s = torch.stack(samples, dim=0)
s = s.to(device)
decoded_samples = model.decoder(s).view(num_samples, 1, 28, 28)

# # Plot the generated images
fig, axes = plt.subplots(2, 5, figsize=(10, 5))

axes = axes.flatten()

for i in range(num_samples):
  axes[i].imshow(np.squeeze(decoded_samples[i].cpu().detach().numpy()), cmap='gray')
  axes[i].set_title(f"Digit {i}", fontsize=18)
  axes[i].axis('off')

plt.tight_layout()
plt.show()

# Step 3: Create the mixture model and draw the samples
num_samples = 10
samples = []

for i in range(num_samples):
  sample = np.random.multivariate_normal(means[i].cpu().detach().numpy(), covs[i].cpu().detach().numpy())
  samples.append(torch.tensor(sample, dtype=torch.float32))

s = torch.stack(samples, dim=0)
s = s.to(device)
decoded_samples = model.decoder(s).view(num_samples, 1, 28, 28)

# # Plot the generated images
fig, axes = plt.subplots(2, 5, figsize=(10, 5))

axes = axes.flatten()

for i in range(num_samples):
  axes[i].imshow(np.squeeze(decoded_samples[i].cpu().detach().numpy()), cmap='gray')
  axes[i].set_title(f"Digit {i}", fontsize=18)
  axes[i].axis('off')

plt.tight_layout()
plt.show()

"""Write your answer in this block

**Answer:**

# **Question 2.** Q-Learning  (*30 total points*)
In this section you will implement the Q-Learning Algorithm to solve "Frozen Lake" problem with the help of [OpenAI’s gym](https://github.com/openai/gym). To install gym, run:
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U gym
# !pip installd -U gym in colab

"""Winter is quickly approaching, and we have to worry about navigating frozen lakes. It’s only early November,
so the lakes haven’t completely frozen and if you make the wrong step you may fall through.
We’ll need to learn how to get to our destination when stuck on the ice, without falling in.
The lake we’re going to consider is a square lake with spots to step on in the shape of a grid. <br>



![alt text](https://raw.githubusercontent.com/chaudatascience/cs599_fall2022/master/ps5/frozen_lake.png)



The surface is described using a 4x4 grid like the following

        S F F F
        F H F H
        F F F H
        H F F G

Each spot can have one of four states:
- S: starting point.
- G: goal point.
- F: frozen spot, where it’s safe to walk.
- H: hole in the ice, where it’s not safe to walk.<br>

There are four possible direction we can move: UP, DOWN, LEFT, RIGHT. Although we can see the path we need to walk, the agent does not. We’re going to train an agent to discover this via problem solving. However, walking on ice isn’t so easy! Sometimes you slip and aren’t able to take the step you intended.

The episode ends when you reach the goal or fall in a hole.

You receive a reward of 1 if you reach the goal, and zero otherwise.

You should take a look at the [frozen_lake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) source code to find how the env is implemented and its detailed documentation.

Then, we can get started.
"""

import gym  ## import Open AI gym

"""
Create a env for Frozen Lake. Note that we have a param named "is_slippery".
    `is_slippery`: Control the randomness of action. If True will move in intended direction with
    probability of 1/3 else will move in either perpendicular direction with
    equal probability of 1/3 in both directions.
       E.g., if action is left and `is_slippery` is True, then:
        - P(move left)=1/3
        - P(move up)=1/3
        - P(move down)=1/3
"""
env = gym.make("FrozenLake-v1", render_mode='ansi', is_slippery=False)


## What are the directions that we can move
print("Action space: ", env.action_space)

## How many positions in the map
print("Observation space: ", env.observation_space)

## Reset the game
env.reset()

## Print out the map and our current position (in red box)
print(env.render())

"""Now, let's make some actions to see how it change the game"""

## In this game, we have 4 actions: 0->3
action_map = {0: "left", 1: "down", 2: "right", 3: "up"}

## Show the game at the beginning
print("Start Screen:")
env.reset()
print(env.render(), end="-----------------------\n\n")

## Try to step left, down, right, up
for action, action_value in action_map.items():

    print(f"If we go {action_value}")

    ## make one step
    observation, reward, terminated, truncated, info = env.step(action)
    print(env.render())

    env.reset()  # restart the game

    print("\n")

"""## **2.1 Short answer:** Frozen Lake Enviroment *(5 pts)*
How many actions and states we have in this game? What are them?

What is terminal state? What are terminal states in this game?

What is the minimum and maximum possible length of reward sequence in this grid world problem?

Write your answer in this block

**Answer:**

## **2.2 Code:** Make n actions *(5 pts)*

Let's define the status of the game:
"""

from enum import Enum

class Status(Enum):
    SUCCEEDED = 1  ## We reached "G"
    FAILED = -1 ## We fell into "H"
    NOT_FINISHED = 0  ## Haven't reached either "G" or "H"

"""Let's say we want to step `n_actions` times, each time we sample a random action.
You'll need to complete the function below.
"""

def question_2_2_make_n_actions(env, n_actions = 10, random_seed = 2022) -> Status:
    """
        Fill in after ##### [YOUR CODE]
        In this function, we make at most `n_actions` steps, each action is random.
        return its status (i.e., Status.SUCCEEDED, Status.FAILED, or Status.NOT_FINISHED)
    """

    env.reset(seed=random_seed)
    env.action_space.seed(random_seed)

    ## The state when we reach "G"
    SUCCESS_STATE =  env.observation_space.n - 1

    for i in range(n_actions):
        print(f"action {i+1}/{n_actions}", )

        ### Your code in this block -------------------------------------------------------

        ## Random an `action` by using "action_space.sample()"
        action = env.action_space.sample() ##### [YOUR CODE]

        ## take the `action` from the previous step by using "step()"
        ## You can refer to the function here:
        # https://github.com/openai/gym/blob/a368cfaaed0b79836f9e5af3c0a9896ef7f7d6ea/gym/wrappers/time_limit.py#L39
        observation, reward, terminated, truncated, info = env.step(action) ##### [YOUR CODE]

        ## render a new map after stepping for displaying
        print(env.render())


        ## determine if we can return at this step (i.e., if we reach G or H)
        if terminated:
            ##### [YOUR CODE]
            if observation == SUCCESS_STATE:
                return Status.SUCCEEDED
            else:
                return Status.FAILED
        ### End of your code --------------------------------------------------------------

    ## Return Status.NOT_FINISHED if we haven't reach G or H yet after `n_actions`
    return Status.NOT_FINISHED



question_2_2_make_n_actions(env, n_actions =10, random_seed=2022)

"""Taking random actions doesn't get us much.

## Hence, We will use Q-learning to solve the problem.

First, let's review **Q-learning (off-policy TD control) Algorithm**

(Fig. 6.12 from [Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf))
![alt text](https://raw.githubusercontent.com/chaudatascience/cs599_fall2022/master/ps5/Q-learning.png)
"""

### We define some helper functions to keep track of the learning

EpisodeStats = namedtuple("Stats", ["episode_lengths", "episode_rewards"])

def show_table(Q, env=env):
    MAP = np.array(env.desc, dtype="<U1").reshape(-1)
    Q_df = pd.concat([pd.DataFrame(np.round(Q,2), columns = ["Left", "Down", "Right", "Up"]),  pd.DataFrame(MAP, columns=["State"])], axis=1)
    Q_df["best_action"] = Q_df.drop(columns = ["State"]).idxmax(axis=1)
    Q_df.loc[(MAP == "H") | (MAP =="G"), "best_action"]=""
    display(Q_df)
    return Q_df



def plot_episode_stats(stats,  smoothing_window=10, graph_name=""):
    # Plot the episode length over time
    fig1 = plt.figure(figsize=(10,5))
    plt.plot(stats.episode_lengths)
    plt.xlabel("Episode")
    plt.ylabel("Episode Length")
    plt.title(f"{graph_name} - Episode Length over Time")

    # Plot the episode reward over time
    fig2 = plt.figure(figsize=(10,5))
    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()
    plt.plot(rewards_smoothed)
    plt.xlabel("Episode")
    plt.ylabel("Episode Reward (Smoothed)")
    plt.title(f"{graph_name} - Episode Reward over Time (Smoothed over window size {smoothing_window})")
    plt.show(fig1)
    plt.show(fig2)
    return fig1, fig2

"""## **2.3 Code:** initialize Q table *(5 pts)*

A Q-Table will give us a score of how good an action is (based on reinforcement learning) at any state. This will help us decide the best action to take from a given state.

Let's make a Q-Table:
For each state, we need a score for each action.

Hence, we need a state_num x action_num table.
"""

def question_2_3_initialize_Q_table(env, init_method: str, random_seed: int) -> np.ndarray:
    """
        Fill in after ##### [YOUR CODE]
        return Q table, numpy array, shape (`state_num`, `action_num`)
    """
    ## set random seed for numpy
    np.random.seed(random_seed)

    ## Get MAP of the game
    MAP  = np.array(env.desc, dtype="<U1").reshape(-1)

    ## number of states and actions
    state_num = env.observation_space.n
    action_num = env.action_space.n


    ### Your code in this block -------------------------------------------------------
    Q = None

    if init_method == "random":
        ## Use np.random.rand to generate random values for Q table.
        ## Then, map all terminal states to 0
        ##### [YOUR CODE]
        Q = np.random.rand(action_num, state_num)
        Q = np.where(MAP == "H", 0, Q)
        Q = np.where(MAP == "G", 0, Q)
        Q = np.transpose(Q)
    elif init_method == "zeros":
        ## Generate all zeros for Q table
        ##### [YOUR CODE]
        Q = np.zeros((state_num, action_num))
    else:
        raise ValueError("method should be 'random' or 'zeros'")

    ##### Return the Q table
    ##### [YOUR CODE]
    return Q
    ### End of your code -------------------------------------------------------------



## test your function
random_seed = 2022
MAP  = np.array(env.desc, dtype="<U1").reshape(-1)
print("MAP:", MAP)
print("\nQ_table (random):\n", question_2_3_initialize_Q_table(env, "random", random_seed))

"""## **2.4 Code:** Q-learning *(5 pts)*

Let's make a Q Table that will help us decide the best action to take from any given state.
"""

def question_2_4_q_learning(env, init_method: str, random_seed: int = 2022, alpha=0.5, gamma=0.95, epsilon=0.1, num_episodes=500):
    """
      return Tuple[Q, stat], where `Q` is the Q table, and `stat` is for statistics
    """
    ### Your code in this block -------------------------------------------------------
    ## Init Q table (re-use from 2.3), name it as `Q`
    Q = question_2_3_initialize_Q_table(env, init_method, random_seed)

    # Keeps track of learning process
    stats = EpisodeStats(
        episode_lengths=np.zeros(num_episodes),
        episode_rewards=np.zeros(num_episodes))

    for i in range(num_episodes):
        state, prob = env.reset(seed=i)

        np.random.seed(i+3)
        env.action_space.seed(i+4)
        random.seed(i+5)

        steps = 0
        terminated = False
        while not terminated:
            if np.random.rand() < 1 - epsilon:
                ##### [YOUR CODE]
                action = np.argmax(Q[state])
            else:
                ##### [YOUR CODE]
                action = env.action_space.sample()

            ## step on the `action`
            ##### [YOUR CODE]
            observation, reward, terminated, truncated, info = env.step(action)

            ## Update value of Q table
            ##### [YOUR CODE]
            # Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.argmax(Q[observation]) - Q[state][action])
            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[observation]) - Q[state, action])
            # print(f'Q[{state}, {action}] = {Q[state,action]}')
            ## Update state
            ##### [YOUR CODE]
            state = observation

            ### End of your code -------------------------------------------------------

            ## Update statistics
            steps += 1
            stats.episode_rewards[i] += reward
            stats.episode_lengths[i] = steps

    return Q, stats

"""## **2.5 Code:** win rate *(5 pts)*

Once we have learned a Q Table that illustrates best actions to take from any given state, let's take those actions and see how often we win.
Let's return both the winrate and the length of the path.
"""

def question_2_5_winrate(Q: np.ndarray, env, num_episodes: int):
    """
        Compute win rate: num wins / num episodes and average number of steps when it wins
        Q: Q table we have learned using Q-learning
        env: Gym env
        num_episodes: number of episodes we run to compute win rate

        return win rate, a float number in [0,1] and average number of steps when it wins
    """


    num_win = 0.
    env.action_space.seed(112022)
    avg_num_steps = []
    for episode in range(num_episodes):
        current_state, _ = env.reset(seed=episode)

        ## set random seed
        np.random.seed(episode)
        env.action_space.seed(episode)
        random.seed(episode)

        # for each episode, we want to know if we win or loose
        ### your code in this block -------------------------------

        terminated = False
        steps = 0

        while not terminated:
          action = Q[current_state].argmax()
          observation, reward, terminated, truncated, info = env.step(action)
          current_state = observation
          steps += 1

        if current_state == env.observation_space.n - 1:
          num_win += 1
          avg_num_steps.append(steps)

        ### End of your code --------------------------------------
    print(f'number of wins: {num_win} out of {num_episodes} total episodes')
    return num_win/num_episodes, np.average(avg_num_steps)

print(4. / 500)

"""**run experiment**"""

def run_experiment(init_method, show_Q_table: bool= True, show_plot: bool= True):
    random_seed = 112022
    n_trials = 10000

    num_episodes_list = [500, 1000, 5000, 10000]
    env = gym.make("FrozenLake-v1", render_mode='ansi').unwrapped


    print(f"{'-'*20} init_method = {init_method} {'-'*20}")
    for n_episodes in num_episodes_list:

        Q, stats = question_2_4_q_learning(env, init_method=init_method, num_episodes=n_episodes)

        run_name = f"{init_method}_{n_episodes}"

        winning_rate = question_2_5_winrate(Q, env, n_trials)

        print(f"\nQ table we learned for init_method={init_method}, n_episodes={n_episodes}:")
        show_table(Q, env)
        plot_episode_stats(stats)

        print(f"winrate, optimal numsteps = {winning_rate}\n-------------------\n")

run_experiment(init_method="zeros", show_Q_table=True, show_plot=True)

## run another experiment with random initialization
run_experiment(init_method="random", show_Q_table=True, show_plot=True)

"""## **2.6 Short answer:** Q table initialization *(5 pts)*

From the results abow, what have you observed?

    Hint: How zero initialization performs when we only use 500 episodes. Does it help if we increase num_episodes?
    Similarly for random initialization, then compare the 2.

Write your answer in this block

**Answer:**

## **2.7 (Bonus)** reward *(10 pts)*

So far, if we reach "G", we get a reward of 1, otherwise 0.
The length of the path makes no difference in terms of reward values (i.e., as long as you reach G without falling into H, we get 1).

Now, similar to question 2.4, but we change the reward a bit:
Let's say we get 1 if reaching "G", "-1" if falling into "H", and a small penalty, such as "-0.001" (play around with this) for "F" to discourage it wandering around.

You will need to complete `q_learning_bonus()` below. You can copy your solution from 2.4, and modify the `reward` part before updating values of Q table.
"""

def q_learning_bonus(env, init_method, random_seed, penalty, alpha=0.5, gamma=0.95, epsilon=0.1, num_episodes=500):
    env.action_space.seed(random_seed)
    np.random.seed(random_seed)

    ###### Your code in this block ---------------------------------

    ## Init Q table, reuse 2.3
    Q = question_2_3_initialize_Q_table(env, init_method=init_method, random_seed=random_seed) ##### [YOUR CODE]


    # Keeps track of learning process
    stats = EpisodeStats(
        episode_lengths=np.zeros(num_episodes),
        episode_rewards=np.zeros(num_episodes))

    for i in range(num_episodes):

        state, prob = env.reset(seed=i)

        ## set random seed to reproduce the results
        np.random.seed(i+3)
        env.action_space.seed(i+4)
        random.seed(i+5)


        steps = 0
        terminated = False
        while not terminated:

            ## Re-use code from question 2.4
            if np.random.rand() < 1 - epsilon:
                action = np.argmax(Q[state])
            else:
                action = env.action_space.sample()

            observation, reward, terminated, truncated, info = env.step(action)

            # if terminated, check to see if the reward needs to be updated
            if terminated:
              if observation != env.observation_space.n - 1:
                reward = -1
            else:
              reward = penalty

            ## Update value of Q table
            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[observation]) - Q[state, action])
            state = observation

            ## End of your code ---------------------------------------

            ## Update statistics
            steps += 1
            stats.episode_rewards[i] += reward
            stats.episode_lengths[i] = steps

    return Q, stats

## Test your function

def run_experiment_bonus(init_method, penalty, show_Q_table: bool= True, show_plot: bool= True):
    """
        you don't need to modify this function
    """

    random_seed = 112022
    n_trials = 10000
    num_episodes_list = [10000]
    env = gym.make("FrozenLake-v1").unwrapped


    print(f"{'-'*20} init_method = {init_method} {'-'*20}")
    for n_episodes in num_episodes_list:

        Q, stats = q_learning_bonus(env, penalty = penalty, init_method=init_method, random_seed=random_seed, num_episodes=n_episodes,)

        run_name = f"{init_method}_{n_episodes}"

        winning_rate = question_2_5_winrate(Q, env, n_trials)

        print(f"\nQ table we learned for init_method={init_method}, n_episodes={n_episodes}:")
        show_table(Q, env)
        plot_episode_stats(stats)

        print(f"winrate, optimal path length = {winning_rate}\n-------------------\n")

init_method = "random"
penalty = -0.01
run_experiment_bonus(init_method=init_method, penalty=penalty)

"""**What effect does it seem to have on the win rate and on the optimal path length?**

Your Answer:



------------

# **Question 3.** Attention  (*25 total points*)

In this question we will look at how we can apply the attention mechanism to a CNN model. We will reuse the MNIST dataset from question 1. In this question we will look at how using attention mechanism with a basic CNN network can affect the performance of our classification task.

## **3.1 Code:** Basic Attention *(10 pts)*

In this question we will provide you with the BasicCNN network that you will work with.

Let us first look at how attention is calculated:

![alt text](https://github.com/ellywang66/CS541/raw/main/attention.png)

For the attention mechanism, the components $Q$, $K$, and $V$ represent the **query**, **key**, and **value** matrices, derived from the input data. Below is a breakdown of each step:

- **$QK^T$**:computes a **similarity score matrix** by taking the dot product between each query and key vector. This matrix measures how each element (token) should focus on every other element in the input sequence.

- **$\sqrt{d_k}$**: Dividing by the square root of $d_k$ scales down the dot product values to stabilize the training process by preventing large values and avoid gradient explosion.

- **softmax**: The softmax function is applied to the scaled similarity scores by converting each value into probability.

- **$V$**: multiply attention weight by $V$ to compute the final attention output.

For basic attention it will attend to elements in one sequence based on their relevance to a specific context or query where 𝑄
 comes from the target sequence and 𝐾,𝑉 are from the source sequence.For this uestion we will simply let query be a **learnable parameter**.
"""

# the BasicCNN model
class BasicCNN(nn.Module):
    def __init__(self):
        super(BasicCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = torch.relu(self.fc1(x))
        return x

import torch
import torch.nn as nn
import torch.optim as optim

# Complete SimpleAttention
class BasicAttention(nn.Module):
    def __init__(self, query_dim, key_dim, value_dim, output_dim):
        super(BasicAttention, self).__init__()

        ## TODO: initialize the query,key and value using linear layers where both the input and output dimension is 128.
        self.query = nn.Linear(query_dim, output_dim)
        self.key = nn.Linear(key_dim, output_dim)
        self.value = nn.Linear(value_dim, output_dim)
        self.softmax = nn.Softmax()



    def forward(self, query_input, key_input, value_input):
        ## TODO: calculate Q, K, and V using  query_input, key_input, value_input.
        # print(query_input.shape)
        # print(key_input.shape)
        # print(value_input.shape)
        Q = self.query(query_input)
        K = self.key(key_input)
        V = self.value(value_input)
        # print(Q.shape)
        # print(K.shape)
        # print(V.shape)

        batch_size = K.shape[0]
        # print(f'batch_size: {batch_size}')

        # expand Q to match batch size
        Q = Q.expand(batch_size, Q.shape[0])
        # print(f'expanded Q shape: {Q.shape}')
        #calculate attention score
        QKT = torch.matmul(Q, torch.transpose(K, 0, 1))
        # print(f'QKT shape: {QKT.shape}')

        attention_scores = self.softmax(QKT / torch.sqrt(torch.tensor(K.shape[1])))
        # print(f'attention_scores shape: {attention_scores.shape}')

        # apply attention weights to V
        output = torch.matmul(attention_scores, V)
        #return attention output

        return output


class CNNWithBasicAttention(nn.Module):
    def __init__(self):
        super(CNNWithBasicAttention, self).__init__()
        self.cnn = BasicCNN()
        ## TODO: initialize self.attention where query, key, value and output dimension are all 128.
        self.attention = BasicAttention(128, 128, 128, 128)

        # Initialize a learnable query vector for basic attention
        self.query = nn.Parameter(torch.randn(128))

        ## Initialize final fully connected layer for the classification head using linear layer.
        self.fc = nn.Linear(128, 10)

    def forward(self, x):
         ## TODO: we will pass the input x through cnn followed by the attention layer and the fully connected layer
        # print(x.shape)
        x = self.cnn(x)
        # print(x.shape)
        x = self.attention(self.query, x, x)
        # print(x.shape)
        x = self.fc(x)
        # print(x.shape)
        return x

tensor = torch.tensor([1, 2, 3, 4])
print(tensor.expand(2, tensor.shape[0]))

def train_model(model, train_loader, criterion, optimizer, epochs=5):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')


def test_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Test Accuracy: {100 * correct / total:.2f}%')

"""We can now test the performance of our model"""

# Train and evaluate the model
model = CNNWithBasicAttention()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
train_model(model, train_loader, criterion, optimizer, epochs=5)
test_model(model, test_loader)
# first run:
# Epoch 1/5, Loss: 2.3006
# Epoch 2/5, Loss: 2.2845
# Epoch 3/5, Loss: 2.2842
# Epoch 4/5, Loss: 2.2823
# Epoch 5/5, Loss: 2.2824
# Test Accuracy: 12.77%

# second run:
# Epoch 1/5, Loss: 2.2921
# Epoch 2/5, Loss: 2.2830
# Epoch 3/5, Loss: 2.2070
# Epoch 4/5, Loss: 2.1280
# Epoch 5/5, Loss: 2.0974
# Test Accuracy: 21.50%

"""## **3.2 Code:** Self-Attention *(10 pts)*

In self attention the attention score is computed solely based on the input sequence without additional learned parameters. The attention socre is computed solely based on the input sequence without any additional learned parameters.
"""

# Step 2: Define a Self-Attention Layer
class SelfAttention(nn.Module):
    def __init__(self, input_dim):
        super(SelfAttention, self).__init__()
        ## TODO: initialize the query,key and value using linear layers where both the input and output dimension is 128.
        self.query = nn.Linear(input_dim, input_dim)
        self.key = nn.Linear(input_dim, input_dim)
        self.value = nn.Linear(input_dim, input_dim)
        self.softmax = nn.Softmax()


    def forward(self, x):
        ## TODO: calculate Q, K, and V using  query_input, key_input, value_input.
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # calculate attention score
        QKT = torch.matmul(Q, torch.transpose(K, 0, 1))
        attention_scores = self.softmax(QKT / torch.sqrt(torch.tensor(K.shape[1])))

        # multiply attention scores by the value matrix
        attention_output = torch.matmul(attention_scores, V)

        # return attention output
        return attention_output


class CNNWithSelfAttention(nn.Module):
    def __init__(self):
        super(CNNWithSelfAttention, self).__init__()
        self.cnn = BasicCNN()
        ## TODO: initialize self.attention where the input dimension 128.
        self.attention = SelfAttention(128)
        ## Initialize final fully connected layer for the classification head using linear layer.
        self.fc = nn.Linear(128, 10)

    def forward(self, x):
        ## TODO: we will pass the input x through cnn followed by the attention layer and the fully connected layer
        x = self.cnn(x)
        x = self.attention(x)
        x = self.fc(x)
        return x

# Instantiate and train the model
model = CNNWithSelfAttention()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Run training and testing
train_model(model, train_loader, criterion, optimizer, epochs=5)
test_model(model, test_loader)

"""## **3.3 Short answer:** Basic attention vs self-attention *(5 pts)*

Compare the performance of basic attention and self-attention. Which one do you think performs better and why?

Write your answer in this block

**Answer:**
"""