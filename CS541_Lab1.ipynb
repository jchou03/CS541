{"cells":[{"cell_type":"markdown","metadata":{"id":"Hbm8sWd0rX-A"},"source":["# CS541: Applied Machine Learning, Fall 2024, Lab 1\n","\n","Lab 1 is an exercise that introduces the [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html#installing-numpy) Python library. When implementing machine learning models, you will often find yourself having to perform complicated operations on matrices or arrays of data and/or model weights.  Numpy implements many high-level mathematical functions that operate on these arrays and matrices.\n","\n","The goal of this is to gain a basic understanding of how Numpy represents matrices and some operations upon them. Many machine learning libraries are also designed to operate like Numpy or are directly implemented using Numpy, making it an essential tool to understand for any practitioner.\n","\n","**Lab Grading**\n","\n","Labs are hands-on exercises designed to provide guided experience in key concepts through this class.  You are graded based on in-lab participation (not correctness), and are not required to submit your lab if you attend in-person.  *Make sure you fill out the attendence form before leaving class*.\n","\n","For students who miss a lab, you can submit a make-up lab on gradescope by the Friday directly following the lab for partial credit.  Please see the syllabus for the lab grading policy."]},{"cell_type":"markdown","metadata":{"id":"HMs9PhKrrX-H"},"source":["## Part 1: Basic Numpy Operations\n","\n","We will begin by learning how to perform some simple operations upon matrices.  Let's say we are given a dataset with three features and five observations.  Each observation can be thought of as a sample in the dataset, whereas the features are variables representing some captured information.  For example, an observation could be an image of a cat, whereas a variable could represent measured values such as the likelihood you see its ears or tail in the photo.  Thus, our dataset would have the following form:\n","\n","<center>\n","$\\begin{bmatrix}\n","0.1 & 0.3 & 0.8\\\\\n","0.4 & 0.7 & 0.6\\\\\n","0.9 & 0.2 & 0.3\\\\\n","0.4 & 0.5 & 0.7\\\\\n","0.2 & 0.9 & 0.5\\\\\n","\\end{bmatrix}\n","$\n","</center>\n","\n","This would be implemented using the [array](https://numpy.org/doc/stable/reference/generated/numpy.array.html) function in Numpy.\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","\n","arr = np.array([[0.1, 0.3, 0.8], [0.4, 0.7, 0.6], [0.9, 0.2, 0.3], [0.4, 0.5, 0.7], [0.2, 0.9, 0.5]])"],"metadata":{"id":"vRVWxgN4UCEt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Slicing an Array:\n","\n","Let's say we decided we wanted to remove the first two samples of the dataset because we found its features were not reliability measured.  For this purpose, we will use [array indexing](https://numpy.org/doc/stable/user/basics.indexing.html) functions.  If we index into the array but leave a “:” after the index number (i.e., “1:”), then it assumes that you want to keep everything afterwards.  If you want to slice off the last item, then you put the colon before the index number.   Let’s try it out!"],"metadata":{"id":"M263MyE7UGHj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dax359TrX-K","outputId":"4adc0941-84cf-482e-f10a-bf61dfe85d9c","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.9 0.2 0.3]\n"," [0.4 0.5 0.7]\n"," [0.2 0.9 0.5]]\n"]}],"source":["# write code to remove the first two observations in the matrix\n","\n","# sliced_arr =\n","print(sliced_arr)\n"]},{"cell_type":"markdown","source":["Notice that we created a new Python variable to represent the sliced matrix.  Thus, our original matrix still contains the entire dataset:"],"metadata":{"id":"vHaj0qexXp22"}},{"cell_type":"code","source":["print(arr.shape)\n","arr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idxVEBazBiT6","outputId":"171a469e-77b0-431e-a635-06df7ad794dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(5, 3)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.1, 0.3, 0.8],\n","       [0.4, 0.7, 0.6],\n","       [0.9, 0.2, 0.3],\n","       [0.4, 0.5, 0.7],\n","       [0.2, 0.9, 0.5]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["Now lets try to remove the first two observations and the last feature:"],"metadata":{"id":"k5fPSS8rYMsy"}},{"cell_type":"code","source":["# write code to remove the first two observations and the last\n","# feature column of the matrix\n","\n","# sliced_arr =\n","print(sliced_arr)"],"metadata":{"id":"YFFbsY61BwQF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"73f23a30-c3ac-4d6a-e66d-6c70d3909ff8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.9 0.2]\n"," [0.4 0.5]\n"," [0.2 0.9]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Vwsbjr7QrX-Q"},"source":["### Searching and Counting:\n","\n","Sometimes we'd like to find features or predictions that represent a particular value.  For this purpose you can use logical functions and functions like \"sum\" or \"count_nonzero\" to implement this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_1RSdz6rX-R","outputId":"bdacbc17-47f5-4404-b289-4e0990b2f648","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}],"source":["arr = np.array([1, 2, 2, 3, 3, 4, 5, 5])\n","\n","# write code to count the number of times \"3\" appears in the\n","# array above\n","\n","#count_of_3 =\n","print(count_of_3)"]},{"cell_type":"markdown","metadata":{"id":"TSsaaj30rX-T"},"source":["### Stacking Arrays:\n","\n","After making predictions on two observations, you may want to stack the vector of predictions to make a single matrix.  Let’s practice that next using the [np.stack](https://numpy.org/doc/stable/reference/generated/numpy.stack.html) function!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZGJlPiyrX-U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0ded2e0-d780-426b-e679-411beb541456"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2 3]\n"," [4 5 6]]\n"]}],"source":["prediction_vector1 = np.array([1, 2, 3])\n","prediction_vector2 = np.array([4, 5, 6])\n","\n","# write code that stacks prediction_vector1 on top of\n","# prediction_vector2\n","#stacked_arr =\n","print(stacked_arr)"]},{"cell_type":"markdown","metadata":{"id":"Gq_jO70LrX-V"},"source":["### Applying Element-wise Functions:\n","\n","Many functions are implemented in numpy to perform elementwise operations.  Let's try using [np.sqrt](https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html) to take the square root of each element of a vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Rw6dFhurX-W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"794db6de-2483-4e0c-f468-0e45a1e37e96"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1.         1.41421356 1.73205081 2.        ]\n"]}],"source":["arr = np.array([1, 2, 3, 4])\n","\n","# write code to perform an elementwise sqrt on \"arr\"\n","#square_root =\n","print(square_root)"]},{"cell_type":"markdown","metadata":{"id":"KSgt189nrX-X"},"source":["### Matrix/Vector Arithmetic\n","\n","Most of the time you should use built-in functions for performing matrix operations.  Experienced practitioners will attempt to vectorize their code, i.e., replacing loops with matrix functions.  However, if you are not experienced with the functions, it can be confusing.  Let’s try to perform a dot product of two vectors.  Since this is written as multiplication, let’s begin by trying to use “*” to represent multiplication.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_97As01WrX-Y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a16168e-45f8-4f41-a499-6bdb24f31a95"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 4 10 18]\n"]}],"source":["vec1 = np.array([1, 2, 3])\n","vec2 = np.array([4, 5, 6])\n","\n","# write code that multiples vec1 by vec2\n","#dot_product_attempt1 =\n","print(dot_product_attempt1)"]},{"cell_type":"markdown","source":["That didn't quite work out! What happened is that in Numpy a \"*\" refers to an elementwise product, i.e., each matrix must be the same size (or it'll throw an error) and then the corresponding entries are multiplied together.  To perform matrix operations, we need to use a function.  Let's try this again using [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)."],"metadata":{"id":"DxBYQxlrwLi6"}},{"cell_type":"code","source":["# write code that uses np.dot to perform a dot\n","# product on vec1 and vec2\n","print(dot_product_attempt2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ot3iX5egwsX0","outputId":"c0922c8d-4145-4d81-8757-39f4abb040c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["32\n"]}]},{"cell_type":"markdown","metadata":{"id":"R404V3kJrX-Z"},"source":["### Cosine similarity\n","\n","Now that we've gone through some basics, let's try to implement a simple formula.  Cosine similarity is often used to measure the distance between two vectors.  Implement the operation using only numpy functions/without a loop.  For reference, here is the formula:\n","\n","<center>\n","$CosSim = \\frac{A \\cdot B}{\\lVert A \\rVert \\lVert B \\rVert}\n","$\n","</center>\n","\n","[Some helpful functions](https://numpy.org/doc/stable/reference/routines.linalg.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1WJR2yvrX-a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6d1be85-631f-419f-dd0c-9650e5d43403"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.9746318461970762\n"]}],"source":["vec1 = np.array([1, 2, 3])\n","vec2 = np.array([4, 5, 6])\n","\n","# write code to compute the cosine similarity of vec1\n","# and vec2\n","#cosine_similarity =\n","print(cosine_similarity)"]},{"cell_type":"markdown","source":["## Part 2: Dataset setup"],"metadata":{"id":"wF2x6ijC6Hw9"}},{"cell_type":"markdown","source":["Before you can implement a machine learning model, we first need to get a dataset within the proper format. In this section, we'll practice some of the basic operations for accomplishing this.\n","\n","As a first step, we need to load the dataset.  In this case let's use a standard dataset, Iris flower, which is composed of attributes of 3 different types of irises'.  As before, the rows are the observations and the columns are composed of different attributes like Sepal length, Petal Length and so on."],"metadata":{"id":"PzDmbJFY6LUg"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","iris = load_iris()\n","\n","# the matrix containing the features of the dataset\n","iris_X = iris.data\n","print('dataset', type(iris_X))\n","\n","# the target labels of the dataset\n","iris_y = iris.target\n","print('labels', type(iris_y))"],"metadata":{"id":"GsRHkz-A7JKL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aff62c15-956c-42eb-946a-85870f2b329f","executionInfo":{"status":"ok","timestamp":1725284140115,"user_tz":240,"elapsed":2997,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset <class 'numpy.ndarray'>\n","labels <class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["### Taking a closer look\n","\n","Upon receiving a new dataset, one of the first things you'll want to do is to get a better understanding of what it contains.  We see above that this dataset was loaded into numpy arrays.  Now, as a first step, write some code to identify how many observations there are in the dataset and what the size of the corresponding label set is as well."],"metadata":{"id":"BO6nIRp-7LhK"}},{"cell_type":"code","source":["# write code to print the size of the dataset and labels\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDjz25KP8aio","outputId":"4f60eaab-542d-43f7-b6e7-d270822906ff","executionInfo":{"status":"ok","timestamp":1725284143257,"user_tz":240,"elapsed":222,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4)\n","(150,)\n"]}]},{"cell_type":"markdown","source":["Now that we see the size of the dataset, we'd like to get a better understanding of what each of the values means.  Luckily this is easily accessable in our 'iris' variable:"],"metadata":{"id":"zlBZ_oZZ1chK"}},{"cell_type":"code","source":["print(\"Feature names:\", iris.feature_names)\n","print(\"Target names:\", iris.target_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJRqSBQL8dsA","outputId":"28d99b09-1c74-4e02-e06b-f1354101a9a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","Target names: ['setosa' 'versicolor' 'virginica']\n"]}]},{"cell_type":"markdown","source":["### Inspecting the data\n","\n","Now that we have an idea of what the size is and what each of the values means, let's take a look at the data itself.  Let's write some code that let's us inspect the first five observations in both the dataset and labels."],"metadata":{"id":"Uf2HH1WO4E5X"}},{"cell_type":"code","source":["# write code to print the first five observations of the dataset\n","# and labels variables\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TtBRCVD8fv4","outputId":"4faef5cf-4028-4863-bff7-dd4e823bd13d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[5.1 3.5 1.4 0.2]\n"," [4.9 3.  1.4 0.2]\n"," [4.7 3.2 1.3 0.2]\n"," [4.6 3.1 1.5 0.2]\n"," [5.  3.6 1.4 0.2]]\n","[2 2 2 2 2]\n"]}]},{"cell_type":"markdown","source":["As we can see the first five labels are all the same.  This suggests that perhaps the dataset has grouped together common classes.  However, just to be sure, let's check how many labels there are:"],"metadata":{"id":"WGVLA1-E2j4o"}},{"cell_type":"code","source":["print(np.unique(iris_y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGJiQwB348jH","outputId":"7ffb8607-e24e-4fca-85ef-32aaf0e9bf77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 2]\n"]}]},{"cell_type":"markdown","source":["Luckily we see that each category is present in the dataset.   This still might mean the data is imbalanced, where some categories are more common, but we will discuss such cases later in the semester."],"metadata":{"id":"_l4Tcqyh3EMt"}},{"cell_type":"markdown","source":["### Split the dataset into train/test/validation\n","\n","Since the dataset looks good so far, the next step we'll want to take is to split the dataset into three parts: training data, testing data, and validation data.  As we'll discuss in class, this is a critical part of setting up a machine learning experiment.  We use only training and validation data to select the right model to use and to set all hyperparameters.  The testing data is only used to verify that the model created using our training and validation data works on unseen samples.  Not following this protocol can be costly as models that are tuned using the test set may not work in practice.  This is often referred to as train-test contamination, and any good ML practitioner must be vigilant in avoiding it.\n","\n","A common way of splitting the data is using 70% training, 20% testing, and 10% validation, but many other splits may be used depending on the dataset.  Let’s implement a function to split our dataset according to these specifications\n"],"metadata":{"id":"RoGcWqnZ4-7l"}},{"cell_type":"code","source":["def get_train_val_test_split(iris_X,iris_y):\n","    # write code to split the dataset into 70/20/10 train/test/val\n","    # Since the dataset labels are in order, you should first randomly\n","    # shuffle the dataset observations, but remember that iris_x and iris_y\n","    # need to be split in the same way so you can't use np.random.shuffle\n","    # directly on iris_X or iris_y\n","\n","\n","    return (train_X,train_y),(val_X,val_y),(test_X,test_y)\n","\n","train_split,val_split,test_split = get_train_val_test_split(iris_X,iris_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CiRDjmf5H7f","outputId":"50e3d4a6-dfb0-4237-df3f-a0f7126cf150","executionInfo":{"status":"ok","timestamp":1725284485924,"user_tz":240,"elapsed":198,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_end: 105\n","val_end: 120\n"]}]},{"cell_type":"code","source":["X_train_split,y_train_split = train_split\n","X_val_split,y_val_split = val_split\n","X_test_split,y_test_split = test_split"],"metadata":{"id":"AlwTRUUY-dtE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's look at the shapes of our data to ensure it's correct"],"metadata":{"id":"OSknqml55KuL"}},{"cell_type":"code","source":["def get_split_shapes(data_split,tag):\n","  print('X' + tag)\n","  print(data_split[0].shape)\n","  print('y' + tag)\n","  print(data_split[1].shape)"],"metadata":{"id":"29feolRP5O8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_split_shapes(train_split,'_train')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6jVuiYa5Qbd","outputId":"6507b64c-a7c7-40d2-f79c-588a72a85ca0","executionInfo":{"status":"ok","timestamp":1725284550942,"user_tz":240,"elapsed":265,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train\n","(105, 4)\n","y_train\n","(105,)\n"]}]},{"cell_type":"code","source":["get_split_shapes(val_split,'_val')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBzsBn0f5Rmo","outputId":"67ec8ad7-a609-48d3-a555-56fa9fc85e28","executionInfo":{"status":"ok","timestamp":1725284557924,"user_tz":240,"elapsed":2,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_val\n","(15, 4)\n","y_val\n","(15,)\n"]}]},{"cell_type":"code","source":["get_split_shapes(test_split,'_test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpHbZyfm5S9c","outputId":"618bc847-7898-483c-a3ba-13a1b465486e","executionInfo":{"status":"ok","timestamp":1725284567239,"user_tz":240,"elapsed":162,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_test\n","(30, 4)\n","y_test\n","(30,)\n"]}]},{"cell_type":"markdown","source":["### Standardize the dataset\n","\n","The final step in setting up a dataset is to standardize it such that each of the features have the same scale (we will discuss why this is important later in the semester).  We will go over many ways of doing this, but one common way is making the data zero-mean and unit variance.  In other words, you want each feature to be subtracted from its mean value across the training set and divided by its standard deviation ([some helpful functions](https://numpy.org/doc/stable/reference/routines.statistics.html))."],"metadata":{"id":"gJ8AmHY94PzM"}},{"cell_type":"code","source":["# Write some code to make each feature to have zero mean and unit variance\n","# across the dataset.  To avoid train-test contamination, compute the mean and\n","# standard deviation using only the training data, but apply those computed\n","# values to the test and validation data as well.\n","\n"],"metadata":{"id":"ZfLGsP6O4NO_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, let's look we look at the normalized data to ensure it looks correct"],"metadata":{"id":"qpFlKNcq4xV3"}},{"cell_type":"code","source":["print(train_iris_X_normalized[:5])\n","print(y_train_split[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K95KM41p_KGO","executionInfo":{"status":"ok","timestamp":1725284904813,"user_tz":240,"elapsed":214,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}},"outputId":"39a16a72-0900-4f29-e86e-c6e8a14933b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.59664569  0.59458007  1.28534363  1.71652492]\n"," [ 0.84377703 -0.10952791  1.17004156  1.31865491]\n"," [ 0.47308001  0.82928273  0.9394374   1.45127824]\n"," [-0.39187971 -1.28304121  0.13232286  0.12504486]\n"," [ 0.59664569 -1.28304121  0.65118221  0.39029154]]\n","[2 2 2 1 1]\n"]}]},{"cell_type":"code","source":["print(val_iris_X_normalized[:5])\n","print(y_train_split[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"syuQxwA9_oCm","executionInfo":{"status":"ok","timestamp":1725284916883,"user_tz":240,"elapsed":154,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}},"outputId":"4ef6ff52-9e20-4863-b67c-c28853302573"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.14474836 -1.28304121  0.70883325  1.05340823]\n"," [-0.88614241  1.53339071 -1.30895311 -1.06856518]\n"," [ 0.34951434 -0.57893323  0.53588013 -0.00757848]\n"," [-0.02118269 -0.57893323  0.76648429  1.58390158]\n"," [-1.50397078  0.82928273 -1.36660415 -1.20118852]]\n","[2 2 2 1 1]\n"]}]},{"cell_type":"code","source":["print(test_iris_X_normalized[:5])\n","print(y_train_split[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMtUaJzX_qfj","executionInfo":{"status":"ok","timestamp":1725284924910,"user_tz":240,"elapsed":179,"user":{"displayName":"Keanu Nichols","userId":"15455753636681602329"}},"outputId":"b07ca3d5-9797-4f19-b522-889708c42fbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.51544538  2.00279603 -1.42425519 -1.06856518]\n"," [-0.02118269  2.23749869 -1.48190622 -1.33381186]\n"," [-0.63901106  1.53339071 -1.30895311 -1.33381186]\n"," [ 0.10238299 -0.10952791  0.24762494  0.39029154]\n"," [ 0.22594866 -0.81363589  0.76648429  0.52291488]]\n","[2 2 2 1 1]\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}